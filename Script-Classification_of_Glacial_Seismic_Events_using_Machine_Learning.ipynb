{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cae12d-d55e-47f4-8e1c-9e4e861e23d1",
   "metadata": {},
   "source": [
    "# jane_Array-MTR\n",
    "written by Dr. Joachim Wassermann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e452fa-6163-422a-9b2a-b15d1a77fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import *\n",
    "from obspy.core.inventory.inventory import Inventory\n",
    "from obspy.core import AttribDict\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.signal.invsim import cosine_taper\n",
    "import obspy_arraytools as AA\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "client = Client(\"http://tarzan\")\n",
    "arraystats = [\"XG.UP1..GLZ\",\"XG.UP2..GLZ\",\"XG.UP3..GLZ\",\"XG.UP4..GLZ\",\"XG.UP5..GLZ\",\"XG.UP6..GLZ\"]\n",
    "\n",
    "\n",
    "ts= t = UTCDateTime(\"2024-03-22T05:00:00\")\n",
    "e = UTCDateTime(\"2024-03-22T08:00:00\")\n",
    "output_path = \"./Grenzgletscher_fk\"\n",
    "figure_path = \"./Grenzgletscher_fk/figure\"\n",
    "fl=1\n",
    "fh=20.00\n",
    "win_len=2.0 \n",
    "win_frac=0.1\n",
    "sll_x=-0.5\n",
    "slm_x=0.5\n",
    "sll_y=-0.5\n",
    "slm_y=0.5\n",
    "sl_s=0.025\n",
    "thres_rel = 0.5\n",
    "\n",
    "while (ts+3600) < e:\n",
    "    start = (ts)\n",
    "    end = (ts+3600)\n",
    "    ts += 3600\n",
    "    try:\n",
    "        sz = Stream()\n",
    "        inv= Inventory()\n",
    "        i = 0\n",
    "        for station in arraystats:\n",
    "            net,stat,loc,chan=station.split('.')\n",
    "            tr = client.get_waveforms(network=net,station=stat,location=loc,channel=chan, starttime=start, endtime=end)\n",
    "            ii = client.get_stations(network=net,station=stat,location='',channel=chan, starttime=start, endtime=end,level=\"response\")\n",
    "            print(tr)\n",
    "            sz += tr\n",
    "            inv += ii\n",
    "        sz.merge()\n",
    "        sz.detrend(\"linear\")\n",
    "        sz.attach_response(inv)\n",
    "        vc = sz.select(component=\"Z\")\n",
    "        array = AA.SeismicArray(\"\",inv)\n",
    "        array.inventory_cull(vc)\n",
    "        print(array.center_of_gravity)\n",
    "        outray = 0. \n",
    "        outray = array.fk_analysis(vc, frqlow=fl, frqhigh=fh, prefilter=True,\\\n",
    "                         static3d=False, array_response=False,vel_corr=4.8, wlen=win_len,\\\n",
    "                         wfrac=win_frac,sec_km=True,\n",
    "                         slx=(sll_x,slm_x),sly=(sll_y,slm_y),\n",
    "                         sls=sl_s)\n",
    "\n",
    "        trace1 = Trace(data=outray.max_rel_power)\n",
    "        trace1.stats.channel = 'REL'\n",
    "        out = outray.max_rel_power\n",
    "\n",
    "        trace2 = Trace(data=outray.max_abs_power)\n",
    "        trace2.stats.channel = 'ABS'\n",
    "        out = np.vstack([out,outray.max_abs_power])\n",
    "\n",
    "        trace3 = Trace(data=outray.max_pow_baz)\n",
    "        trace3.stats.channel = 'BACK'\n",
    "        out = np.vstack([out,outray.max_pow_baz])\n",
    "\n",
    "        trace4 = Trace(data=outray.max_pow_slow)\n",
    "        trace4.stats.channel = 'SLOW'\n",
    "        out = np.vstack([out,outray.max_pow_slow])\n",
    "\n",
    "        #saving f-k analysis results into mseed file\n",
    "        fk = Stream()\n",
    "        tr = Trace()\n",
    "\n",
    "        delta = outray.timestep\n",
    "\n",
    "        tr.stats.network = outray.inventory.networks[0].code\n",
    "        tr.stats.station = outray.inventory.networks[0][0].code\n",
    "        tr.stats.channel = \"ZGC\"\n",
    "        tr.stats.location = \"\"\n",
    "        tr.data = outray.max_rel_power\n",
    "        tr.stats.starttime = outray.starttime\n",
    "        tr.stats.delta = delta\n",
    "\n",
    "        fk += tr\n",
    "\n",
    "        tr = Trace()\n",
    "        tr.stats.network = outray.inventory.networks[0].code\n",
    "        tr.stats.station = outray.inventory.networks[0][0].code\n",
    "        tr.stats.channel = \"ZGI\"\n",
    "        tr.stats.location = \"\"\n",
    "        tr.stats.starttime = outray.starttime\n",
    "        tr.data = outray.max_abs_power\n",
    "        tr.stats.delta = delta\n",
    "\n",
    "        fk += tr\n",
    "\n",
    "        tr = Trace()\n",
    "        tr.stats.network = outray.inventory.networks[0].code\n",
    "        tr.stats.station = outray.inventory.networks[0][0].code\n",
    "        tr.stats.channel = \"ZGS\"\n",
    "        tr.stats.location = \"\"\n",
    "        tr.stats.starttime = outray.starttime\n",
    "        tr.data = outray.max_pow_baz\n",
    "        tr.stats.delta = delta\n",
    "\n",
    "        fk += tr\n",
    "\n",
    "        tr = Trace()\n",
    "        tr.stats.network = outray.inventory.networks[0].code\n",
    "        tr.stats.station = outray.inventory.networks[0][0].code\n",
    "        tr.stats.channel = \"ZGA\"\n",
    "        tr.stats.location = \"\"\n",
    "        tr.stats.starttime = outray.starttime\n",
    "        tr.data = outray.max_pow_slow\n",
    "        tr.stats.delta = delta\n",
    "\n",
    "        fk += tr\n",
    "\n",
    "        myday = \"%03d\"%fk[0].stats.starttime.julday\n",
    "\n",
    "        pathyear = str(fk[0].stats.starttime.year)\n",
    "        # open catalog file in read and write mode in case we are continuing d/l,\n",
    "        # so we can append to the file\n",
    "        mydatapath = os.path.join(output_path, pathyear)\n",
    "\n",
    "        # create datapath \n",
    "        if not os.path.exists(mydatapath):\n",
    "            os.mkdir(mydatapath)\n",
    "\n",
    "        mydatapath = os.path.join(mydatapath, fk[0].stats.network)\n",
    "        if not os.path.exists(mydatapath):\n",
    "            os.mkdir(mydatapath)\n",
    "\n",
    "        mydatapath = os.path.join(mydatapath, fk[0].stats.station)\n",
    "\n",
    "        # create datapath \n",
    "        if not os.path.exists(mydatapath):\n",
    "                os.mkdir(mydatapath)\n",
    "\n",
    "\n",
    "        for tr in fk:\n",
    "            print(\"saving to \" + mydatapath)\n",
    "            print(tr)\n",
    "            mydatapathchannel = os.path.join(mydatapath,tr.stats.channel + \".D\")\n",
    "\n",
    "            if not os.path.exists(mydatapathchannel):\n",
    "                os.mkdir(mydatapathchannel)\n",
    "\n",
    "            netFile = tr.stats.network + \".\" + tr.stats.station +  \".\" + tr.stats.location + \".\" + tr.stats.channel+ \".D.\" + pathyear + \".\" + myday\n",
    "            netFileout = os.path.join(mydatapathchannel, netFile)\n",
    "\n",
    "            # try to open File\n",
    "            print(netFileout)\n",
    "            try:\n",
    "                netFileout = open(netFileout, 'ab')\n",
    "            except:\n",
    "                netFileout = open(netFileout, 'w')\n",
    "            tr.write(netFileout , format='MSEED',encoding=\"FLOAT64\")\n",
    "            netFileout.close()\n",
    "\n",
    "        #print(outray)\n",
    "\n",
    "        # Plot FK\n",
    "        labels = ['ref','rel.power', 'abs.power', 'baz', 'slow']\n",
    "        xlocator = mdates.AutoDateLocator()\n",
    "        fig = plt.figure()\n",
    "        alphas = out[0,:]\n",
    "        condition1 = (out[0,:] < thres_rel)\n",
    "        condition2 = (out[3,:] > 0.4) \n",
    "        tt = np.ma.masked_array(fk[0].times(\"matplotlib\"),mask=condition1)\n",
    "        tt = np.ma.masked_array(tt,mask=condition2)\n",
    "        axis = []\n",
    "\n",
    "        for i, lab in enumerate(labels):\n",
    "            try:\n",
    "                if i == 0:\n",
    "                    ax = fig.add_subplot(5, 1, i + 1,sharex=None)\n",
    "                    ax.plot(vc[0].times(\"matplotlib\"),vc[0].data)\n",
    "                else:\n",
    "                    ax = fig.add_subplot(5, 1, i + 1,sharex=axis[0])\n",
    "                    mask_v = np.ma.masked_array(out[i-1,:],mask=condition1)\n",
    "                    mask_v = np.ma.masked_array(mask_v,mask=condition2)\n",
    "                    ax.scatter(tt,mask_v, c=out[0,:], alpha=alphas,\n",
    "                       edgecolors='none', cmap=cm.viridis_r)\n",
    "                    ax.set_ylabel(lab)\n",
    "                    ax.set_ylim(mask_v.min()-0.1, mask_v.max()+0.1)\n",
    "                    ax.xaxis.set_major_locator(xlocator)\n",
    "                    ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(xlocator))\n",
    "                axis.append(ax)\n",
    "            except Exception as er:\n",
    "                sys.stderr.write(\"Error:\" + str(er))\n",
    "                traceback.print_exc()\n",
    "        fig.suptitle( 'jane-fk %s' % ( start ))\n",
    "        fig.autofmt_xdate()\n",
    "        fig.subplots_adjust(left=0.15, top=0.95, right=0.95, bottom=0.2, hspace=0)\n",
    "        plt.savefig(\"%s/FK-%s.png\"%(figure_path,start.strftime('%Y-%m-%dT%H')))\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fae737-706a-467b-9163-2194e512aa93",
   "metadata": {},
   "source": [
    "# FK_trig\n",
    "written by Dr. Joachim Wassermann, adapted by Nicole Katrin Richels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5f56e-1500-46ba-90b7-7fdf819e3988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from obspy import *\n",
    "from obspy.clients.filesystem import sds\n",
    "from obspy.clients.fdsn import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import matplotlib as mpl\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Set better default style for matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['axes.facecolor'] = 'white'\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# Add logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "out_file = \"./Grenzgletscher_fk/fk_trigger.csv\"\n",
    "root_dir = \"./Grenzgletscher_fk/\"\n",
    "save_path = \"./Grenzgletscher_fk/trig_figs/\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "cl = sds.Client(sds_root=root_dir)\n",
    "\n",
    "ostart = start = UTCDateTime(2024, 3, 19)\n",
    "end = UTCDateTime(2024, 3, 23)\n",
    "trig_times = []\n",
    "\n",
    "# Debug info\n",
    "logger.info(f\"Starting trigger detection from {ostart} to {end}\")\n",
    "trigger_count = 0\n",
    "\n",
    "# Thresholds for detection\n",
    "relp_threshold = 0.6\n",
    "slow_threshold = 0.5\n",
    "min_trigger_separation = 2  # Minimum seconds between triggers\n",
    "\n",
    "last_trigger_abs_time = None\n",
    "\n",
    "while start + 1*3600 < end:\n",
    "    endy = start + 1*3600\n",
    "    try:\n",
    "        logger.info(f\"Processing window: {start} - {endy}\")\n",
    "        st = cl.get_waveforms(network=\"XG\", station=\"UP1\", location=\"\", channel=\"ZG?\", starttime=start, endtime=endy)\n",
    "        st.merge()\n",
    "        \n",
    "        # Check for required channels\n",
    "        channels = [tr.stats.channel for tr in st]\n",
    "        if \"ZGC\" not in channels or \"ZGA\" not in channels:\n",
    "            logger.warning(f\"Missing required channels. Available: {channels}\")\n",
    "            start += 1*3600\n",
    "            continue\n",
    "        \n",
    "        relp = st.select(channel=\"ZGC\")[0]\n",
    "        slow = st.select(channel=\"ZGA\")[0]\n",
    "        \n",
    "        window_triggers = 0\n",
    "        \n",
    "        for i in range(1, relp.stats.npts):\n",
    "            # Check if conditions are met\n",
    "            if relp.data[i] > relp_threshold and slow.data[i] < slow_threshold:\n",
    "                # Check for state transition\n",
    "                if i > 5 and (\n",
    "                    (np.mean(relp.data[i-1:i]) < relp_threshold) or \n",
    "                    (np.mean(slow.data[i-1:i]) > slow_threshold)\n",
    "                ):\n",
    "                    trig_time = relp.times(reftime=ostart)[i]\n",
    "                    abs_time = ostart + trig_time\n",
    "                    \n",
    "                    # Check minimum separation\n",
    "                    if last_trigger_abs_time is None or (abs_time - last_trigger_abs_time) > min_trigger_separation:\n",
    "                        trig_times.append(trig_time)\n",
    "                        logger.info(f\"Trigger detected at index {i}: relative time={trig_time}, absolute time={abs_time}\")\n",
    "                        window_triggers += 1\n",
    "                        last_trigger_abs_time = abs_time\n",
    "                    else:\n",
    "                        logger.info(f\"Skipping close trigger at {abs_time} (too close to previous)\")\n",
    "        \n",
    "        logger.info(f\"Found {window_triggers} triggers in this window\")\n",
    "        trigger_count += window_triggers\n",
    "        start += 1*3600\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing window {start}-{endy}: {e}\")\n",
    "        start += 1*3600\n",
    "        continue\n",
    "\n",
    "logger.info(f\"Total triggers detected: {trigger_count}\")\n",
    "logger.info(f\"Writing triggers to {out_file}\")\n",
    "\n",
    "# Add validation before writing to CSV\n",
    "valid_triggers = []\n",
    "for j in trig_times:\n",
    "    trigger_time = ostart + j\n",
    "    valid_triggers.append(trigger_time)\n",
    "\n",
    "# Write triggers to file\n",
    "with open(out_file, \"w\") as fo:\n",
    "    for trigger_time in valid_triggers:\n",
    "        fo.write(\"%s\\n\" % trigger_time)\n",
    "\n",
    "logger.info(f\"Written {len(valid_triggers)} triggers to CSV file\")\n",
    "\n",
    "# Process individual events\n",
    "all_baz = []\n",
    "all_slow = []\n",
    "all_rp = []\n",
    "clw = Client(\"http://tarzan.geophysik.uni-muenchen.de\")\n",
    "\n",
    "# Define the global colormap\n",
    "global_cmap = plt.cm.viridis\n",
    "\n",
    "# Define wave velocities for incidence angle calculation\n",
    "p_velocity = 3.8  # km/s for P-waves\n",
    "s_velocity = 1.8  # km/s for S-waves\n",
    "\n",
    "logger.info(f\"Processing {len(trig_times)} individual events\")\n",
    "\n",
    "#Track successful plot creation\n",
    "successful_plots = 0\n",
    "failed_plots = 0\n",
    "\n",
    "for j_idx, j in enumerate(trig_times):\n",
    "    try:\n",
    "        event_time = ostart + j\n",
    "        logger.info(f\"Processing event {j_idx+1}/{len(trig_times)} at {event_time}\")\n",
    "        \n",
    "        # Get waveform data with a window around the event time\n",
    "        try:\n",
    "            st = clw.get_waveforms(network=\"XG\", station=\"UP1\", location=\"\", channel=\"??Z\", \n",
    "                                   starttime=(event_time-1), endtime=event_time+10)\n",
    "            ar = cl.get_waveforms(network=\"XG\", station=\"UP1\", location=\"\", channel=\"ZG?\", \n",
    "                                  starttime=(event_time-1), endtime=event_time+10)\n",
    "        except Exception as data_error:\n",
    "            logger.error(f\"Failed to get waveform data for event at {event_time}: {data_error}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "        \n",
    "        # Check if valid data\n",
    "        if len(st) == 0 or len(ar) == 0:\n",
    "            logger.warning(f\"No data found for event at {event_time}, skipping\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # Check for required channels\n",
    "        ar_channels = [tr.stats.channel for tr in ar]\n",
    "        if \"ZGC\" not in ar_channels or \"ZGA\" not in ar_channels or \"ZGS\" not in ar_channels:\n",
    "            logger.warning(f\"Missing required array channels for event at {event_time}, skipping. Available: {ar_channels}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # Check if there's a valid vertical component\n",
    "        if not st.select(component=\"Z\"):\n",
    "            logger.warning(f\"No vertical component found for event at {event_time}, skipping\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # Process waveforms\n",
    "        st.detrend(\"linear\")\n",
    "        st.taper(type='cosine', max_percentage=0.05)\n",
    "        st.filter(\"bandpass\", freqmin=1, freqmax=20)\n",
    "\n",
    "        # Extract data\n",
    "        if ar.select(channel=\"ZGC\"):\n",
    "            rel_power = ar.select(channel=\"ZGC\")[0].data\n",
    "            all_rp.append(rel_power)\n",
    "        else:\n",
    "            logger.warning(f\"Missing ZGC channel for event at {event_time}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        if ar.select(channel=\"ZGS\"):\n",
    "            baz = ar.select(channel=\"ZGS\")[0].data\n",
    "            all_baz.append(baz)\n",
    "        else:\n",
    "            logger.warning(f\"Missing ZGS channel for event at {event_time}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        if ar.select(channel=\"ZGA\"):\n",
    "            slow = ar.select(channel=\"ZGA\")[0].data\n",
    "            all_slow.append(slow)\n",
    "        else:\n",
    "            logger.warning(f\"Missing ZGA channel for event at {event_time}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # Calculate incidence angles based on slowness values\n",
    "        incidence_angles = []\n",
    "        wave_types = []\n",
    "        \n",
    "        for s in slow:\n",
    "            if s < 0.3:  # P-wave region\n",
    "                sin_i = min(p_velocity * s, 0.99)\n",
    "                angle = np.degrees(np.arcsin(sin_i))\n",
    "                wave_type = \"P\"\n",
    "            elif s <= 0.6:  # S-wave region\n",
    "                if s_velocity * s > 0.99:\n",
    "                    # Scale between 60-85 degrees based on the slowness value\n",
    "                    angle = 60 + 25 * (s - 0.2) / 0.3\n",
    "                else:\n",
    "                    sin_i = min(s_velocity * s, 0.99)\n",
    "                    angle = np.degrees(np.arcsin(sin_i))\n",
    "                wave_type = \"S\"\n",
    "            else:\n",
    "                # For values outside velocity model assumptions\n",
    "                angle = np.nan\n",
    "                wave_type = \"Unknown\"\n",
    "                \n",
    "            incidence_angles.append(angle)\n",
    "            wave_types.append(wave_type)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        incidence_angles = np.array(incidence_angles)\n",
    "        wave_types = np.array(wave_types)\n",
    "        \n",
    "        # Close any existing figures\n",
    "        plt.close('all')\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(12, 16), dpi=100)\n",
    "        fig.suptitle(f\"Seismic Event Analysis - {event_time.strftime('%Y-%m-%d %H:%M:%S')}\", \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Create a grid layout with 3 rows and 2 columns\n",
    "        gs = plt.GridSpec(3, 2, figure=fig, height_ratios=[1, 1, 1], width_ratios=[1, 1],\n",
    "                          hspace=0.35, wspace=0.35)\n",
    "        \n",
    "        common_time_limits = None\n",
    "        if st.select(component=\"Z\"):\n",
    "            vert_tr = st.select(component=\"Z\")[0]\n",
    "            time_data = vert_tr.times(\"matplotlib\")\n",
    "            \n",
    "            # Get time limits for alignment\n",
    "            end_time = max(time_data)\n",
    "            start_time = min(time_data)\n",
    "            \n",
    "            # Common time limits for all time-based plots\n",
    "            common_time_limits = [start_time, end_time]\n",
    "            \n",
    "            # Create the time locator for all plots\n",
    "            seconds_locator = mdates.SecondLocator(interval=1)\n",
    "            seconds_formatter = mdates.DateFormatter('%H:%M:%S')\n",
    "        else:\n",
    "            logger.warning(\"No vertical component data for establishing time limits\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "        \n",
    "        # PLOT 1: Seismogram - Row 1, Col 1 (Upper Left)\n",
    "        axtrace = fig.add_subplot(gs[0, 0])\n",
    "        \n",
    "        # Plot the vertical component data\n",
    "        if st.select(component=\"Z\"):\n",
    "            axtrace.plot(time_data, vert_tr.data, 'k', linewidth=1.2)\n",
    "            axtrace.ticklabel_format(axis='y', style='sci', scilimits=(-2,2))\n",
    "            axtrace.set_ylabel('Amplitude [mm/s]', color='k', fontsize=11, fontweight='bold')\n",
    "            axtrace.set_title('Vertical Component Waveform', fontsize=12, fontweight='bold', pad=10)\n",
    "            \n",
    "            # Set x-axis ticks every second\n",
    "            axtrace.xaxis.set_major_locator(seconds_locator)\n",
    "            axtrace.xaxis.set_major_formatter(seconds_formatter)\n",
    "            \n",
    "            # Set time limits\n",
    "            axtrace.set_xlim(common_time_limits)\n",
    "            \n",
    "            # Enhance grid for seismogram\n",
    "            axtrace.grid(True, which='both', axis='x', color='gray', alpha=0.5, linestyle='-')\n",
    "            axtrace.grid(True, which='major', axis='y', color='gray', alpha=0.5, linestyle='-')\n",
    "            \n",
    "            # Rotate time labels\n",
    "            plt.setp(axtrace.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        else:\n",
    "            logger.warning(\"No vertical component data for trace plot\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # PLOT 2: Polar Plot - Row 1, Col 2 (Upper Right)\n",
    "        polar_ax = fig.add_subplot(gs[0, 1], projection='polar')\n",
    "        \n",
    "        # Check for the necessary data\n",
    "        if len(baz) > 0 and len(slow) > 0 and len(rel_power) > 0:\n",
    "            # Convert backazimuth to radians\n",
    "            baz_rad = np.radians(baz)\n",
    "            baz_rad[baz_rad < 0] += 2*np.pi\n",
    "            baz_rad[baz_rad > 2*np.pi] -= 2*np.pi\n",
    "            \n",
    "            # Create 2D histogram for polar plot\n",
    "            N = int(360./5.)  # 5-degree bins\n",
    "            abins = np.arange(N + 1) * 2*np.pi / N\n",
    "            sbins = np.linspace(0, 0.4, 20) \n",
    "            \n",
    "            hist, baz_edges, sl_edges = np.histogram2d(baz_rad, slow, bins=[abins, sbins], weights=rel_power)\n",
    "            \n",
    "            # Create meshgrid for pcolormesh\n",
    "            A, S = np.meshgrid(abins, sbins)\n",
    "\n",
    "            polar_ax.set_theta_zero_location(\"N\")\n",
    "            polar_ax.set_theta_direction(-1)\n",
    "            \n",
    "            # Use pcolormesh for polar plot with the global colormap\n",
    "            pcm = polar_ax.pcolormesh(A, S, hist.T, cmap=global_cmap, alpha=0.7, shading='auto')\n",
    "            \n",
    "            # Improve polar plot settings\n",
    "            polar_ax.grid(True, linewidth=1.5)\n",
    "            \n",
    "            # Add radial labels\n",
    "            polar_ax.set_rticks([0.1, 0.2, 0.3, 0.4])\n",
    "            polar_ax.set_rlabel_position(135)\n",
    "            polar_ax.set_rmax(0.4)\n",
    "            polar_ax.set_title('Polar Plot: Backazimuth vs. Slowness', fontsize=12, fontweight='bold', pad=15)\n",
    "        else:\n",
    "            logger.warning(\"Missing data for polar plot\")\n",
    "            \n",
    "        # PLOT 3: Spectrogram - Row 2, Col 1 (Middle Left)\n",
    "        axspec = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "        # Get the vertical component data for spectrogram\n",
    "        if st.select(component=\"Z\"):\n",
    "            tr = st.select(component=\"Z\")[0]\n",
    "    \n",
    "            try:\n",
    "                # Calculate spectrogram\n",
    "                specgram = tr.spectrogram(wlen=0.5, per_lap=0.9, show=False, axes=axspec)\n",
    "        \n",
    "                # Limit frequency range\n",
    "                axspec.set_ylim(1, 25)  # Limit frequency to 1-25 Hz\n",
    "        \n",
    "                # Set labels and grid\n",
    "                axspec.set_ylabel('Frequency [Hz]', fontsize=11, fontweight='bold')\n",
    "                \n",
    "                # Clear the current x-axis labels and ticks\n",
    "                axspec.set_xticklabels([])\n",
    "                axspec.set_xticks([])\n",
    "                \n",
    "                # Create secondary axis that matches seismogram time\n",
    "                ax2 = axspec.twiny()\n",
    "                ax2.set_xlim(common_time_limits)\n",
    "                ax2.xaxis.set_major_locator(seconds_locator)\n",
    "                ax2.xaxis.set_major_formatter(seconds_formatter)\n",
    "                ax2.xaxis.tick_bottom()\n",
    "                ax2.xaxis.set_label_position('bottom')\n",
    "                ax2.tick_params(axis='x', pad=10)\n",
    "                \n",
    "                # Rotate time labels\n",
    "                plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "                \n",
    "                # Add horizontal grid lines\n",
    "                axspec.grid(True, which='major', axis='y', color='gray', alpha=0.01, linestyle='-')\n",
    "                \n",
    "                # Add vertical grid lines\n",
    "                for pos in ax2.get_xticks():\n",
    "                    axspec.axvline(pos, color='gray', alpha=0.01, linestyle='-')\n",
    "                \n",
    "                # Set title above the plot\n",
    "                axspec.set_title('Spectrogram', fontsize=12, fontweight='bold', pad=10)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating spectrogram: {e}\")\n",
    "        else:\n",
    "            logger.warning(\"No vertical component data for spectrogram\")\n",
    "            \n",
    "        # PLOT 4: Incidence Angle - Row 2, Col 2 (Middle Right)\n",
    "        axangle = fig.add_subplot(gs[1, 1])\n",
    "        \n",
    "        # Only plot valid incidence angles\n",
    "        valid_mask = ~np.isnan(incidence_angles)\n",
    "        if any(valid_mask):\n",
    "            # Get matplotlib times for the incidence angle data\n",
    "            angle_times = ar.select(channel=\"ZGA\")[0].times(\"matplotlib\")\n",
    "            \n",
    "            # Calculate point sizes based on relative power if not already defined\n",
    "            rel_power_norm = rel_power / np.max(rel_power) if np.max(rel_power) > 0 else np.zeros_like(rel_power)\n",
    "            sizes = 20 + 100 * rel_power_norm\n",
    "            \n",
    "            # Use the same marker sizing and coloring scheme as the other plots\n",
    "            scatter_angle = axangle.scatter(\n",
    "                angle_times[valid_mask], \n",
    "                incidence_angles[valid_mask],\n",
    "                c=rel_power[valid_mask], \n",
    "                cmap=global_cmap, \n",
    "                s=sizes[valid_mask], \n",
    "                alpha=0.7\n",
    "            )\n",
    "            \n",
    "            # Add markers to indicate wave type\n",
    "            p_mask = np.logical_and(valid_mask, np.array(wave_types) == \"P\")\n",
    "            if any(p_mask):\n",
    "                axangle.scatter(\n",
    "                    angle_times[p_mask], \n",
    "                    incidence_angles[p_mask],\n",
    "                    s=30, alpha=0.7, facecolors='none', edgecolors='blue',\n",
    "                    linewidth=1.5, marker='o', label='P-wave'\n",
    "                )\n",
    "                \n",
    "            s_mask = np.logical_and(valid_mask, np.array(wave_types) == \"S\")\n",
    "            if any(s_mask):\n",
    "                axangle.scatter(\n",
    "                    angle_times[s_mask], \n",
    "                    incidence_angles[s_mask],\n",
    "                    s=30, alpha=0.7, facecolors='none', edgecolors='red',\n",
    "                    linewidth=1.5, marker='s', label='S-wave'\n",
    "                )\n",
    "            \n",
    "            axangle.set_ylabel('Incidence Angle [deg]', fontsize=10, fontweight='bold')\n",
    "            axangle.set_title('Incidence Angle vs. Time', fontsize=11, fontweight='bold', pad=10)\n",
    "            \n",
    "            # Set x-axis ticks every second\n",
    "            axangle.xaxis.set_major_locator(seconds_locator)\n",
    "            axangle.xaxis.set_major_formatter(seconds_formatter)\n",
    "            \n",
    "            # Set time limits to match seismogram\n",
    "            axangle.set_xlim(common_time_limits)\n",
    "            \n",
    "            # Set reasonable y-limits for the plot\n",
    "            axangle.set_ylim(0, 90)\n",
    "            \n",
    "            # Enhanced grid with lines every second\n",
    "            axangle.grid(True, which='major', axis='both', color='gray', alpha=0.5, linestyle='-')\n",
    "            axangle.legend(loc='upper right', fontsize=9)\n",
    "            \n",
    "            # Rotate time labels\n",
    "            plt.setp(axangle.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        else:\n",
    "            logger.warning(\"No valid incidence angle data for plot\")\n",
    "        \n",
    "        # PLOT 5: Backazimuth - Row 3, Col 1 (Lower Left)\n",
    "        axbaz = fig.add_subplot(gs[2, 0])\n",
    "        \n",
    "        # Get matplotlib times for the backazimuth data\n",
    "        if ar.select(channel=\"ZGS\") and len(ar.select(channel=\"ZGS\")[0].data) > 0:\n",
    "            baz_times = ar.select(channel=\"ZGS\")[0].times(\"matplotlib\")\n",
    "            \n",
    "            # Check for matching data lengths\n",
    "            if len(baz_times) == len(baz) and len(baz) == len(rel_power):\n",
    "                scatter_baz = axbaz.scatter(baz_times, baz, \n",
    "                           c=rel_power, cmap=global_cmap, s=sizes, alpha=0.7)\n",
    "                axbaz.set_ylabel('Backazimuth [deg]', fontsize=11, fontweight='bold')\n",
    "                axbaz.set_xlabel('Time (UTC)', fontsize=11, fontweight='bold')\n",
    "                axbaz.set_ylim(0, 360)\n",
    "                axbaz.set_yticks([0, 90, 180, 270, 360])\n",
    "                axbaz.set_title('Backazimuth vs. Time', fontsize=12, fontweight='bold', pad=10)\n",
    "                \n",
    "                # Set x-axis ticks every second\n",
    "                axbaz.xaxis.set_major_locator(seconds_locator)\n",
    "                axbaz.xaxis.set_major_formatter(seconds_formatter)\n",
    "                \n",
    "                # Set time limits to match seismogram\n",
    "                axbaz.set_xlim(common_time_limits)\n",
    "                \n",
    "                # Enhanced grid with lines\n",
    "                axbaz.grid(True, which='major', axis='both', color='gray', alpha=0.5, linestyle='-')\n",
    "                \n",
    "                # Rotate time labels\n",
    "                plt.setp(axbaz.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "            else:\n",
    "                logger.warning(f\"Data length mismatch in backazimuth plot\")\n",
    "        else:\n",
    "            logger.warning(\"No backazimuth data available for plot\")\n",
    "        \n",
    "        # PLOT 6: Slowness - Row 3, Col 2 (Lower Right)\n",
    "        axslow = fig.add_subplot(gs[2, 1])\n",
    "        \n",
    "        # Add horizontal line for the threshold\n",
    "        axslow.axhline(y=slow_threshold, color='r', linestyle='--', alpha=0.7, \n",
    "                       label=f'Threshold ({slow_threshold})')\n",
    "        \n",
    "        # Get matplotlib times for the slowness data\n",
    "        if ar.select(channel=\"ZGA\") and len(ar.select(channel=\"ZGA\")[0].data) > 0:\n",
    "            slow_times = ar.select(channel=\"ZGA\")[0].times(\"matplotlib\")\n",
    "            \n",
    "            # Check for matching data lengths\n",
    "            if len(slow_times) == len(slow) and len(slow) == len(rel_power):\n",
    "                # Use scatter for slowness, sized by rel_power like backazimuth\n",
    "                scatter_slow = axslow.scatter(slow_times, slow, \n",
    "                            c=rel_power, cmap=global_cmap, s=sizes, alpha=0.7)\n",
    "                \n",
    "                axslow.set_ylabel('Slowness [s/km]', fontsize=11, fontweight='bold')\n",
    "                axslow.set_xlabel('Time (UTC)', fontsize=11, fontweight='bold')\n",
    "                axslow.set_title('Slowness vs. Time', fontsize=12, fontweight='bold', pad=10)\n",
    "                \n",
    "                # Set x-axis ticks every second\n",
    "                axslow.xaxis.set_major_locator(seconds_locator)\n",
    "                axslow.xaxis.set_major_formatter(seconds_formatter)\n",
    "                \n",
    "                # Set time limits to match seismogram\n",
    "                axslow.set_xlim(common_time_limits)\n",
    "                \n",
    "                # Set y-axis limits\n",
    "                axslow.set_ylim(0, max(1.0, np.max(slow)*1.1))\n",
    "                \n",
    "                # Enhanced grid with lines\n",
    "                axslow.grid(True, which='major', axis='both', color='gray', alpha=0.5, linestyle='-')\n",
    "                \n",
    "                axslow.legend(loc='upper right', fontsize=9)\n",
    "                \n",
    "                # Rotate time labels\n",
    "                plt.setp(axslow.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "            else:\n",
    "                logger.warning(f\"Data length mismatch in slowness plot\")\n",
    "        else:\n",
    "            logger.warning(\"No slowness data available for plot\")\n",
    "        \n",
    "        # Colorbar for all plots using the same colormap\n",
    "        if 'pcm' in locals():\n",
    "            cax = fig.add_axes([0.93, 0.3, 0.02, 0.4])  # Position for vertical colorbar\n",
    "            cbar = fig.colorbar(pcm, cax=cax)\n",
    "            cbar.set_label('Relative Power', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Save figure\n",
    "        fmt = \"png\"\n",
    "        filename = f'{save_path}UP1_{event_time.strftime(\"%Y%m%d_%H%M%S\")}_array.{fmt}'\n",
    "        \n",
    "\n",
    "        try:\n",
    "            plt.savefig(filename, format=fmt, dpi=300)\n",
    "            logger.info(f\"Saved figure: {filename}\")\n",
    "            successful_plots += 1\n",
    "        except Exception as save_error:\n",
    "            logger.error(f\"Failed to save figure {filename}: {save_error}\")\n",
    "            failed_plots += 1\n",
    "        \n",
    "        plt.close(\"all\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing event at {event_time}: {e}\")\n",
    "        failed_plots += 1\n",
    "        plt.close(\"all\")  # Make sure to close all figures even in case of error\n",
    "\n",
    "logger.info(f\"Processing complete! Successfully created {successful_plots} plots, {failed_plots} failed\")\n",
    "logger.info(f\"Summary: {len(trig_times)} triggers detected, {len(valid_triggers)} written to CSV, {successful_plots} plots created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b548645-4d74-46c3-bbbe-5d9938fac3da",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98499000-ef51-4a1a-87dc-11542f58fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "from obspy.clients.filesystem import sds\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import hilbert\n",
    "import warnings\n",
    "from obspy.clients.fdsn import Client\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define parameters\n",
    "root_dir = \"./Grenzgletscher_fk/\"\n",
    "input_file = \"./Grenzgletscher_fk/2025_fk_trigger.csv\"\n",
    "output_file = \"./Grenzgletscher_fk/2025_features.csv\"\n",
    "window_before = 1    # seconds before trigger\n",
    "window_after = 5     # seconds after trigger\n",
    "\n",
    "# Velocity model parameters\n",
    "p_velocity = 3.8  # km/s for P-waves\n",
    "s_velocity = 1.8  # km/s for S-waves\n",
    "\n",
    "cl = sds.Client(sds_root=root_dir)\n",
    "clw = Client(\"http://tarzan.geophysik.uni-muenchen.de\")\n",
    "\n",
    "def calculate_incidence_angles_from_slowness(slowness_data):\n",
    "    \"\"\"Calculate incidence angles from slowness values using velocity model\"\"\"\n",
    "    if slowness_data is None or len(slowness_data) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    incidence_angles = []\n",
    "    wave_types = []\n",
    "    \n",
    "    for s in slowness_data:\n",
    "        if np.isnan(s):\n",
    "            incidence_angles.append(np.nan)\n",
    "            wave_types.append(\"Unknown\")\n",
    "            continue\n",
    "            \n",
    "        if s < 0.3:  # P-wave region\n",
    "            sin_i = min(p_velocity * s, 0.99)\n",
    "            angle = np.degrees(np.arcsin(sin_i))\n",
    "            wave_type = \"P\"\n",
    "        elif s <= 0.6:  # S-wave region\n",
    "            if s_velocity * s > 0.99:\n",
    "                # Scale between 60-85 degrees based on the slowness value\n",
    "                angle = 60 + 25 * (s - 0.2) / 0.3\n",
    "            else:\n",
    "                sin_i = min(s_velocity * s, 0.99)\n",
    "                angle = np.degrees(np.arcsin(sin_i))\n",
    "            wave_type = \"S\"\n",
    "        else:\n",
    "            # For values outside velocity model assumptions\n",
    "            angle = np.nan\n",
    "            wave_type = \"Unknown\"\n",
    "            \n",
    "        incidence_angles.append(angle)\n",
    "        wave_types.append(wave_type)\n",
    "    \n",
    "    return np.array(incidence_angles), np.array(wave_types)\n",
    "\n",
    "def map_array_to_waveform_indices(array_data, array_sr, waveform_sr, waveform_length):\n",
    "    # Map array processing data to waveform sample indices using interpolation\n",
    "    if array_data is None or len(array_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        array_times = np.arange(len(array_data)) / array_sr\n",
    "        waveform_times = np.arange(waveform_length) / waveform_sr\n",
    "        \n",
    "        if len(array_data) == 1:\n",
    "            return np.full(waveform_length, array_data[0])\n",
    "        \n",
    "        interp_func = interp1d(array_times, array_data, kind='linear', \n",
    "                              fill_value='extrapolate', bounds_error=False)\n",
    "        mapped_data = interp_func(waveform_times)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        if np.any(np.isnan(mapped_data)):\n",
    "            mask = ~np.isnan(mapped_data)\n",
    "            if np.any(mask):\n",
    "                first_valid = np.where(mask)[0][0]\n",
    "                mapped_data[:first_valid] = mapped_data[first_valid]\n",
    "                for i in range(1, len(mapped_data)):\n",
    "                    if np.isnan(mapped_data[i]):\n",
    "                        mapped_data[i] = mapped_data[i-1]\n",
    "        \n",
    "        return mapped_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error in array data mapping: {e}\")\n",
    "        return np.full(waveform_length, np.mean(array_data))\n",
    "\n",
    "def analyze_slowness_during_signal(ar_slowness, ar_back_azimuth, ar_incidence_angle, \n",
    "                                 envelope, trigger_sample, sample_rate):\n",
    "\n",
    "   # Analyze slowness changes from trigger until amplitude drops below 2% of max.\n",
    "    features = {}\n",
    "    \n",
    "    if ar_slowness is None or len(ar_slowness) == 0:\n",
    "        # No slowness data available\n",
    "        for key in ['slowness_min_during_signal', 'slowness_mean_during_signal', \n",
    "                   'slowness_std_during_signal', 'slowness_change_during_signal',\n",
    "                   'back_azimuth_at_min_slowness', 'incidence_angle_at_min_slowness']:\n",
    "            features[key] = np.nan\n",
    "        return features\n",
    "    \n",
    "    # Find signal activity window\n",
    "    envelope_max = np.max(envelope)\n",
    "    threshold = envelope_max * 0.02  # 2% threshold\n",
    "    \n",
    "    # Find start and end of signal activity\n",
    "    above_threshold = envelope > threshold\n",
    "    if not np.any(above_threshold):\n",
    "        # No significant signal\n",
    "        for key in ['slowness_min_during_signal', 'slowness_mean_during_signal', \n",
    "                   'slowness_std_during_signal', 'slowness_change_during_signal',\n",
    "                   'back_azimuth_at_min_slowness', 'incidence_angle_at_min_slowness']:\n",
    "            features[key] = np.nan\n",
    "        return features\n",
    "    \n",
    "    signal_indices = np.where(above_threshold)[0]\n",
    "    signal_start = max(0, signal_indices[0])\n",
    "    signal_end = min(len(ar_slowness), signal_indices[-1] + 1)\n",
    "    \n",
    "    # Extract slowness during signal activity\n",
    "    slowness_during_signal = ar_slowness[signal_start:signal_end]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_mask = ~np.isnan(slowness_during_signal)\n",
    "    if not np.any(valid_mask):\n",
    "        for key in ['slowness_min_during_signal', 'slowness_mean_during_signal', \n",
    "                   'slowness_std_during_signal', 'slowness_change_during_signal',\n",
    "                   'back_azimuth_at_min_slowness', 'incidence_angle_at_min_slowness']:\n",
    "            features[key] = np.nan\n",
    "        return features\n",
    "    \n",
    "    valid_slowness = slowness_during_signal[valid_mask]\n",
    "    valid_indices = np.where(valid_mask)[0] + signal_start\n",
    "    \n",
    "    # Slowness features during signal\n",
    "    features['slowness_min_during_signal'] = np.min(valid_slowness)\n",
    "    features['slowness_mean_during_signal'] = np.mean(valid_slowness)\n",
    "    features['slowness_std_during_signal'] = np.std(valid_slowness)\n",
    "    features['slowness_change_during_signal'] = np.max(valid_slowness) - np.min(valid_slowness)\n",
    "    \n",
    "    # Find index of minimum slowness\n",
    "    min_slowness_idx_in_valid = np.argmin(valid_slowness)\n",
    "    min_slowness_global_idx = valid_indices[min_slowness_idx_in_valid]\n",
    "    \n",
    "    # Back azimuth at minimum slowness\n",
    "    if (ar_back_azimuth is not None and len(ar_back_azimuth) > min_slowness_global_idx):\n",
    "        features['back_azimuth_at_min_slowness'] = ar_back_azimuth[min_slowness_global_idx]\n",
    "    else:\n",
    "        features['back_azimuth_at_min_slowness'] = np.nan\n",
    "    \n",
    "    # Incidence angle at minimum slowness\n",
    "    if (ar_incidence_angle is not None and len(ar_incidence_angle) > min_slowness_global_idx):\n",
    "        features['incidence_angle_at_min_slowness'] = ar_incidence_angle[min_slowness_global_idx]\n",
    "    else:\n",
    "        features['incidence_angle_at_min_slowness'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features(trace, ar_back_azimuth, ar_incidence_angle, ar_slowness, trigger_time):\n",
    "    # Extract all required features from the data\n",
    "    \n",
    "    if trace is None or not hasattr(trace, 'data') or len(trace.data) == 0:\n",
    "        return {}\n",
    "    \n",
    "    data = trace.data.astype(np.float64)\n",
    "    data = data - np.mean(data)  # Remove DC\n",
    "    sample_rate = trace.stats.sampling_rate\n",
    "    features = {}\n",
    "    \n",
    "    # Basic waveform features\n",
    "    features['waveform_rms'] = np.sqrt(np.mean(np.square(data)))\n",
    "    features['waveform_peak_to_peak'] = np.max(data) - np.min(data)\n",
    "    features['waveform_abs_energy'] = np.sum(np.abs(data))\n",
    "    features['waveform_max_abs'] = np.max(np.abs(data))\n",
    "    features['waveform_mean_abs'] = np.mean(np.abs(data))\n",
    "    features['waveform_skewness'] = skew(data)\n",
    "    features['waveform_kurtosis'] = kurtosis(data)\n",
    "    features['waveform_std'] = np.std(data)\n",
    "    \n",
    "    # Zero crossing rate\n",
    "    zero_crossings = np.where(np.diff(np.signbit(data)))[0]\n",
    "    features['zero_crossing_rate'] = len(zero_crossings) / (len(data) / sample_rate)\n",
    "    \n",
    "    # Spectral features\n",
    "    if len(data) >= 4:\n",
    "        windowed_data = data * signal.windows.hann(len(data))\n",
    "        fft = np.abs(np.fft.rfft(windowed_data))\n",
    "        freqs = np.fft.rfftfreq(len(data), d=1.0/sample_rate)\n",
    "        \n",
    "        if len(fft) > 1:\n",
    "            fft = fft[1:]  # Remove DC\n",
    "            freqs = freqs[1:]\n",
    "        \n",
    "        if len(fft) > 0 and np.sum(fft) > 1e-10:\n",
    "            total_power = np.sum(fft)\n",
    "            features['spectral_centroid'] = np.sum(freqs * fft) / total_power\n",
    "            features['dominant_freq'] = freqs[np.argmax(fft)]\n",
    "            features['spectral_spread'] = np.sqrt(np.sum(((freqs - features['spectral_centroid']) ** 2) * fft) / total_power)\n",
    "            \n",
    "            # Spectral rolloff\n",
    "            cumulative_power = np.cumsum(fft)\n",
    "            rolloff_idx = np.where(cumulative_power >= 0.85 * total_power)[0]\n",
    "            features['spectral_rolloff'] = freqs[rolloff_idx[0]] if len(rolloff_idx) > 0 else freqs[-1]\n",
    "            \n",
    "            # Spectral flatness\n",
    "            geometric_mean = np.exp(np.mean(np.log(fft + 1e-10)))\n",
    "            features['spectral_flatness'] = geometric_mean / np.mean(fft)\n",
    "            \n",
    "            # Frequency band ratios\n",
    "            fft_power = fft**2\n",
    "            total_power_squared = np.sum(fft_power)\n",
    "            for low, high in [(1, 5), (5, 10), (10, 15), (15, 20)]:\n",
    "                band_indices = np.logical_and(freqs >= low, freqs <= high)\n",
    "                if np.any(band_indices):\n",
    "                    band_energy = np.sum(fft_power[band_indices])\n",
    "                    features[f'spec_band_{low}_{high}_ratio'] = band_energy / total_power_squared\n",
    "                else:\n",
    "                    features[f'spec_band_{low}_{high}_ratio'] = 0.0\n",
    "        else:\n",
    "            # Default spectral features\n",
    "            for key in ['spectral_centroid', 'dominant_freq', 'spectral_spread', 'spectral_rolloff', 'spectral_flatness']:\n",
    "                features[key] = 0.0\n",
    "            for low, high in [(1, 5), (5, 10), (10, 15), (15, 20)]:\n",
    "                features[f'spec_band_{low}_{high}_ratio'] = 0.0\n",
    "    \n",
    "    # Envelope and duration features\n",
    "    try:\n",
    "        analytic_signal = hilbert(data)\n",
    "        envelope = np.abs(analytic_signal)\n",
    "        \n",
    "        # Smooth envelope\n",
    "        from scipy.ndimage import uniform_filter1d\n",
    "        smooth_window = max(1, int(sample_rate * 0.05))\n",
    "        smoothed_envelope = uniform_filter1d(envelope, size=smooth_window)\n",
    "        \n",
    "        envelope_max = np.max(smoothed_envelope)\n",
    "        \n",
    "        if envelope_max > 0:\n",
    "            # Duration features with 5% and 2% thresholds\n",
    "            for threshold, name in [(0.05, '5_percent'), (0.02, '2_percent')]:\n",
    "                envelope_threshold = envelope_max * threshold\n",
    "                above_threshold = smoothed_envelope > envelope_threshold\n",
    "                \n",
    "                if np.any(above_threshold):\n",
    "                    duration_indices = np.where(above_threshold)[0]\n",
    "                    duration_sec = (duration_indices[-1] - duration_indices[0]) / sample_rate\n",
    "                    features[f'duration_{name}'] = duration_sec\n",
    "                else:\n",
    "                    features[f'duration_{name}'] = 0.0\n",
    "            \n",
    "            # Rise time\n",
    "            above_start = smoothed_envelope > (envelope_max * 0.05)\n",
    "            above_end = smoothed_envelope > (envelope_max * 0.8)\n",
    "            \n",
    "            if np.any(above_start) and np.any(above_end):\n",
    "                rise_start_idx = np.where(above_start)[0][0]\n",
    "                rise_end_idx = np.where(above_end)[0][0]\n",
    "                features['envelope_rise_time'] = max(0, (rise_end_idx - rise_start_idx) / sample_rate)\n",
    "            else:\n",
    "                features['envelope_rise_time'] = 0.0\n",
    "            \n",
    "            # Peak position and envelope shape\n",
    "            peak_idx = np.argmax(smoothed_envelope)\n",
    "            features['peak_position_normalized'] = peak_idx / len(smoothed_envelope)\n",
    "            features['envelope_skewness'] = skew(smoothed_envelope)\n",
    "            features['envelope_kurtosis'] = kurtosis(smoothed_envelope)\n",
    "        else:\n",
    "            # Zero envelope case\n",
    "            for name in ['5_percent', '2_percent']:\n",
    "                features[f'duration_{name}'] = 0.0\n",
    "            features['envelope_rise_time'] = 0.0\n",
    "            features['peak_position_normalized'] = 0.5\n",
    "            features['envelope_skewness'] = 0.0\n",
    "            features['envelope_kurtosis'] = 0.0\n",
    "        \n",
    "        # Improved slowness analysis during signal activity\n",
    "        trigger_sample = int(window_before * sample_rate)\n",
    "        slowness_features = analyze_slowness_during_signal(\n",
    "            ar_slowness, ar_back_azimuth, ar_incidence_angle,\n",
    "            smoothed_envelope, trigger_sample, sample_rate\n",
    "        )\n",
    "        features.update(slowness_features)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error computing envelope/slowness features: {e}\")\n",
    "        # Set defaults\n",
    "        for name in ['5_percent', '2_percent']:\n",
    "            features[f'duration_{name}'] = 0.0\n",
    "        features['envelope_rise_time'] = 0.0\n",
    "        features['peak_position_normalized'] = 0.5\n",
    "        features['envelope_skewness'] = 0.0\n",
    "        features['envelope_kurtosis'] = 0.0\n",
    "        for key in ['slowness_min_during_signal', 'slowness_mean_during_signal', \n",
    "                   'slowness_std_during_signal', 'slowness_change_during_signal',\n",
    "                   'back_azimuth_at_min_slowness', 'incidence_angle_at_min_slowness']:\n",
    "            features[key] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "def main():\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Load trigger times\n",
    "    try:\n",
    "        if input_file.endswith('.csv'):\n",
    "            df = pd.read_csv(input_file)\n",
    "            time_column = df.columns[0]\n",
    "            trigger_times = [UTCDateTime(ts) for ts in df[time_column]]\n",
    "        else:\n",
    "            with open(input_file, 'r') as f:\n",
    "                trigger_times = [UTCDateTime(line.strip()) for line in f if line.strip()]\n",
    "        logger.info(f\"Loaded {len(trigger_times)} trigger times\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading trigger file: {e}\")\n",
    "        return\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    # Process each event\n",
    "    for i, event_time in enumerate(trigger_times):\n",
    "        try:\n",
    "            logger.info(f\"Processing event {i+1}/{len(trigger_times)} at {event_time}\")\n",
    "            event_features = {\"event_time\": str(event_time)}\n",
    "\n",
    "            # Get waveform data\n",
    "            try:\n",
    "                st = clw.get_waveforms(network=\"4D\", station=\"A2P1\", location=\"*\", channel=\"??Z\", \n",
    "                                       starttime=(event_time-window_before), \n",
    "                                       endtime=event_time+window_after)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error getting waveform data: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get array data\n",
    "            try:\n",
    "                ar = cl.get_waveforms(network=\"4D\", station=\"A2P1\", location=\"*\", channel=\"ZG?\", \n",
    "                                     starttime=(event_time-window_before), \n",
    "                                     endtime=event_time+window_after)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error getting array data: {e}\")\n",
    "                ar = []\n",
    "            \n",
    "            if len(st) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Preprocess seismic waveforms only\n",
    "            st.detrend(\"linear\")\n",
    "            st.taper(type='cosine', max_percentage=0.05)\n",
    "            st.filter(\"bandpass\", freqmin=1, freqmax=20)\n",
    "            \n",
    "            vert_traces = st.select(component=\"Z\")\n",
    "            if len(vert_traces) == 0:\n",
    "                continue\n",
    "                \n",
    "            vert_tr = vert_traces[0]\n",
    "            \n",
    "            # Process array data\n",
    "            back_azimuth = incidence_angle = slowness = None\n",
    "            if len(ar) > 0:\n",
    "                for channel_code, var_name in [(\"ZGS\", \"back_azimuth\"), (\"ZGA\", \"slowness\")]:\n",
    "                    traces = [tr for tr in ar if tr.stats.channel == channel_code]\n",
    "                    if traces:\n",
    "                        raw_data = traces[0].data\n",
    "                        raw_sr = traces[0].stats.sampling_rate\n",
    "                        mapped_data = map_array_to_waveform_indices(\n",
    "                            raw_data, raw_sr, vert_tr.stats.sampling_rate, len(vert_tr.data)\n",
    "                        )\n",
    "                        if var_name == \"back_azimuth\":\n",
    "                            back_azimuth = mapped_data\n",
    "                        elif var_name == \"slowness\":\n",
    "                            slowness = mapped_data\n",
    "                \n",
    "                # Calculate incidence angles from slowness instead of reading from ZGI\n",
    "                if slowness is not None:\n",
    "                    incidence_angle, wave_types = calculate_incidence_angles_from_slowness(slowness)\n",
    "            \n",
    "            # Extract features\n",
    "            extracted_features = extract_features(vert_tr, back_azimuth, incidence_angle, slowness, event_time)\n",
    "            \n",
    "            if extracted_features:\n",
    "                event_features.update(extracted_features)\n",
    "                all_features.append(event_features)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing event at {event_time}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    if all_features:\n",
    "        df = pd.DataFrame(all_features)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"Saved {len(all_features)} feature sets to {output_file}\")\n",
    "        logger.info(f\"Total features: {len(df.columns)-1}\")\n",
    "    else:\n",
    "        logger.error(\"No features extracted\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc69ac4-df5b-4ae7-9d11-c27f378d90f1",
   "metadata": {},
   "source": [
    "# Feature Plots\n",
    "## Assign class names manually in a column named class_names before running this\n",
    "### The file is called features_labeled.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe5109-7d06-4556-a636-ee823e5d6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from matplotlib.colors import to_rgba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "output_dir = \"./Grenzgletscher_fk/plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def clean_numeric_value(value):\n",
    "    \"\"\"Clean numeric values that might have periods as thousand separators\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return np.nan\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        # Remove periods used as thousand separators, but keep decimal points\n",
    "        # First, check if it's scientific notation\n",
    "        if 'E' in value.upper():\n",
    "            try:\n",
    "                return float(value.replace(',', '.'))\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "        # Count periods to determine if they're thousand separators\n",
    "        period_count = value.count('.')\n",
    "        if period_count > 1:\n",
    "            # Multiple periods likely means thousand separators\n",
    "            # Keep only the last period as decimal point\n",
    "            parts = value.split('.')\n",
    "            if len(parts) > 1:\n",
    "                integer_part = ''.join(parts[:-1])\n",
    "                decimal_part = parts[-1]\n",
    "                cleaned_value = f\"{integer_part}.{decimal_part}\"\n",
    "            else:\n",
    "                cleaned_value = value\n",
    "        else:\n",
    "            cleaned_value = value\n",
    "        \n",
    "        try:\n",
    "            return float(cleaned_value)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    return float(value) if not pd.isna(value) else np.nan\n",
    "\n",
    "def load_data(filepath):\n",
    "    print(f\"Loading data from {filepath}\")\n",
    "    \n",
    "    try:\n",
    "        # Try reading with different separators\n",
    "        if filepath.endswith('.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, sep=';')\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_csv(filepath, sep=',')\n",
    "                except:\n",
    "                    df = pd.read_csv(filepath, sep='\\t')\n",
    "        else:\n",
    "            df = pd.read_csv(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Handle class column naming\n",
    "    if 'class_name' in df.columns and 'class' not in df.columns:\n",
    "        df = df.rename(columns={'class_name': 'class'})\n",
    "    \n",
    "    # Clean numeric columns\n",
    "    numeric_columns = []\n",
    "    for col in df.columns:\n",
    "        if col not in ['event_time', 'class', 'class_name']:\n",
    "            try:\n",
    "                # Try to convert the first few non-null values\n",
    "                sample_values = df[col].dropna().head(10)\n",
    "                cleaned_sample = [clean_numeric_value(val) for val in sample_values]\n",
    "                if any(not pd.isna(val) for val in cleaned_sample):\n",
    "                    print(f\"Cleaning numeric column: {col}\")\n",
    "                    df[col] = df[col].apply(clean_numeric_value)\n",
    "                    numeric_columns.append(col)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not convert column {col} to numeric: {e}\")\n",
    "    \n",
    "    print(f\"Successfully converted {len(numeric_columns)} numeric columns\")\n",
    "    \n",
    "    # Handle time column\n",
    "    if 'event_time' in df.columns:\n",
    "        try:\n",
    "            df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "            df = df.sort_values('event_time')\n",
    "            \n",
    "            first_time = df['event_time'].min()\n",
    "            df['time_delta_min'] = (df['event_time'] - first_time).dt.total_seconds() / 60\n",
    "            df['time_delta_sec'] = (df['event_time'] - first_time).dt.total_seconds()\n",
    "            df['time_delta_days'] = (df['event_time'] - first_time).dt.total_seconds() / (60*60*24)\n",
    "            \n",
    "            df['day_of_year'] = df['event_time'].dt.dayofyear\n",
    "            df['hour_of_day'] = df['event_time'].dt.hour\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process event_time: {e}\")\n",
    "            df['event_time'] = pd.date_range(start='2024-03-19', periods=len(df), freq='H')\n",
    "            df['time_delta_min'] = np.arange(len(df))\n",
    "            df['time_delta_sec'] = np.arange(len(df)) * 60\n",
    "            df['time_delta_days'] = np.arange(len(df)) / 24\n",
    "    else:\n",
    "        df['event_time'] = pd.date_range(start='2024-03-19', periods=len(df), freq='H')\n",
    "        df['time_delta_min'] = np.arange(len(df))\n",
    "        df['time_delta_sec'] = np.arange(len(df)) * 60\n",
    "        df['time_delta_days'] = np.arange(len(df)) / 24\n",
    "    \n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    print(f\"Classes found: {sorted(df['class'].unique()) if 'class' in df.columns else 'No class column'}\")\n",
    "    print(f\"Loaded {len(df)} events with {len(df.columns)} features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_class_colors(classes):\n",
    "    color_palette = {\n",
    "        0: 'tab:blue',\n",
    "        1: 'tab:orange', \n",
    "        2: 'tab:green',\n",
    "        3: 'tab:red',\n",
    "        4: 'tab:purple',\n",
    "        5: 'tab:brown',\n",
    "        6: 'tab:pink',\n",
    "        7: 'tab:gray',\n",
    "        8: 'tab:olive',\n",
    "        9: 'tab:cyan'\n",
    "    }\n",
    "    \n",
    "    # Handle string class names\n",
    "    unique_classes = sorted(classes)\n",
    "    if isinstance(unique_classes[0], str):\n",
    "        color_map = {}\n",
    "        colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', \n",
    "                 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "        for i, cls in enumerate(unique_classes):\n",
    "            color_map[cls] = colors[i % len(colors)]\n",
    "        return color_map\n",
    "    else:\n",
    "        return {cls: color_palette[i % 10] for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "def save_plot(fig, filename):\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "def plot_boxplots_for_all_features(df):\n",
    "    exclude_cols = ['event_time', 'time_delta_min', 'time_delta_sec', 'time_delta_days', \n",
    "                    'day_of_year', 'hour_of_day', 'class', 'class_name']\n",
    "    feature_cols = [col for col in df.columns \n",
    "                   if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    if not feature_cols:\n",
    "        print(\"No numeric feature columns found for box plots!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Creating box plots for {len(feature_cols)} features\")\n",
    "    \n",
    "    classes = sorted(df['class'].unique())\n",
    "    class_colors = get_class_colors(classes)\n",
    "    \n",
    "    # Create box plots for each feature across all classes\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        print(f\"Processing feature {i+1}/{len(feature_cols)}: {col}\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Prepare data for each class (using absolute values)\n",
    "        class_data = []\n",
    "        class_labels = []\n",
    "        for cls in classes:\n",
    "            data = df[df['class'] == cls][col].dropna()\n",
    "            if len(data) > 0:\n",
    "                class_data.append(np.abs(data.values))  # Take absolute values\n",
    "                # Use class names directly, just capitalize them\n",
    "                class_labels.append(str(cls).capitalize())\n",
    "        \n",
    "        if not class_data:\n",
    "            print(f\"No data found for feature {col}\")\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "        \n",
    "        # Create box plot\n",
    "        try:\n",
    "            bplot = ax.boxplot(class_data, \n",
    "                      patch_artist=True,\n",
    "                      notch=False,\n",
    "                      showfliers=True,\n",
    "                      widths=0.6,\n",
    "                      medianprops={'color': 'red', 'linewidth': 1.5},\n",
    "                      whiskerprops={'linewidth': 1.2},\n",
    "                      capprops={'linewidth': 1.2},\n",
    "                      boxprops={'linewidth': 1.2})\n",
    "            \n",
    "            # Color the boxes according to class colors\n",
    "            for j, box in enumerate(bplot['boxes']):\n",
    "                if j < len(classes):\n",
    "                    box_color = class_colors[classes[j]]\n",
    "                    box.set(facecolor=box_color, alpha=0.6)\n",
    "            \n",
    "            # Add scatter points with slight jitter for better distribution visibility\n",
    "            for j, cls in enumerate(classes):\n",
    "                if j >= len(class_data):\n",
    "                    continue\n",
    "                y = class_data[j]  # Already absolute values from above\n",
    "                if len(y) > 0:\n",
    "                    # Limit number of points for readability\n",
    "                    if len(y) > 1000:\n",
    "                        idx = np.random.choice(len(y), 1000, replace=False)\n",
    "                        y = y[idx]\n",
    "                    \n",
    "                    x = np.random.normal(j+1, 0.08, size=len(y))\n",
    "                    ax.scatter(x, y, alpha=0.4, s=8, c=class_colors[cls], edgecolor='none')\n",
    "            \n",
    "            ax.set_title(f'Distribution of |{col}| by Class', fontsize=14, fontweight='bold')\n",
    "            ax.set_ylabel(f'|{col}|', fontsize=12)\n",
    "            ax.set_xlabel('Class', fontsize=12)\n",
    "            ax.set_xticklabels(class_labels)\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Add statistics text\n",
    "            stats_text = f\"Total samples: {len(df)}\\n\"\n",
    "            for j, cls in enumerate(classes):\n",
    "                count = len(df[df['class'] == cls])\n",
    "                stats_text += f\"{str(cls).capitalize()}: {count} samples\\n\"\n",
    "            \n",
    "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Clean filename\n",
    "            clean_col_name = col.replace('/', '_').replace(' ', '_').replace('(', '').replace(')', '')\n",
    "            save_plot(fig, f'boxplot_{clean_col_name}.png')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating box plot for {col}: {e}\")\n",
    "            plt.close(fig)\n",
    "    \n",
    "    # Create summary statistics (using absolute values)\n",
    "    try:\n",
    "        print(\"\\nCreating summary statistics...\")\n",
    "        # Apply absolute values to feature columns for summary statistics\n",
    "        abs_df = df.copy()\n",
    "        for col in feature_cols:\n",
    "            abs_df[col] = np.abs(df[col])\n",
    "        \n",
    "        summary_stats = abs_df[feature_cols + ['class']].groupby('class').agg(['mean', 'std', 'median', 'min', 'max'])\n",
    "        summary_stats.to_csv(os.path.join(output_dir, 'summary_statistics_by_class_absolute.csv'))\n",
    "        print(\"Summary statistics (absolute values) saved to summary_statistics_by_class_absolute.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating summary statistics: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Path to the labeled features CSV file\n",
    "    input_file = \"./Grenzgletscher_fk/known_features_labeled.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Error: File {input_file} not found!\")\n",
    "        return\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(input_file)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"Failed to load data!\")\n",
    "        return\n",
    "    \n",
    "    # Check if we have a class column\n",
    "    if 'class' not in df.columns:\n",
    "        print(\"Error: No 'class' column found in the data!\")\n",
    "        return\n",
    "    \n",
    "    # Run box plot visualization\n",
    "    print(\"Starting box plot creation...\")\n",
    "    plot_boxplots_for_all_features(df)\n",
    "    \n",
    "    print(\"Box plot visualizations completed!\")\n",
    "    print(f\"All plots saved to: {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9dca1-8b27-4f18-9f70-612e3fbbae0e",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "The Random Forest tutorial with the original code was written by Michaela Wenner and presented at the 9th Munich Earth Skience School (MESS 2019). The material is available online at https://github.com/krischer/mess_2019/blob/master/3_wednesday/random_forest.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3dac5-c477-4a70-ae39-b532d7cbf12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"./Grenzgletscher_fk/features_labeled.csv\", sep=\";\")\n",
    "\n",
    "# Split features and labels\n",
    "label_column = \"class_name\"\n",
    "\n",
    "# Drop the event_time column - we don't want time as a feature\n",
    "X = df.drop(columns=[label_column, \"event_time\"])\n",
    "y = df[label_column]\n",
    "\n",
    "# Analyze class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "print(class_counts)\n",
    "print(f\"\\nClass percentages:\")\n",
    "class_percentages = (class_counts / len(y) * 100).round(2)\n",
    "print(class_percentages)\n",
    "\n",
    "# Identify classes with very few samples (less than 2% of data or less than 10 samples)\n",
    "min_samples_threshold = max(10, len(y) * 0.02)\n",
    "rare_classes = class_counts[class_counts < min_samples_threshold].index.tolist()\n",
    "if rare_classes:\n",
    "    print(f\"\\nWarning: Classes with very few samples (< {min_samples_threshold:.0f}): {rare_classes}\")\n",
    "    print(\"Consider combining these with similar classes or collecting more data.\")\n",
    "\n",
    "# Encode labels if they're words\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "class_names = le.classes_\n",
    "\n",
    "print(f\"\\nClasses found: {class_names}\")\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = dict(zip(np.unique(y_encoded), class_weights))\n",
    "print(f\"\\nClass weights for balancing:\")\n",
    "for i, weight in class_weight_dict.items():\n",
    "    print(f\"  {class_names[i]}: {weight:.3f}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in train/test sets\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "train_class_counts = pd.Series(y_train).value_counts()\n",
    "for i, count in train_class_counts.items():\n",
    "    print(f\"  {class_names[i]}: {count}\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "test_class_counts = pd.Series(y_test).value_counts()\n",
    "for i, count in test_class_counts.items():\n",
    "    print(f\"  {class_names[i]}: {count}\")\n",
    "\n",
    "# Train the Random Forest classifier with class balancing\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=300,  \n",
    "    max_depth=25,      \n",
    "    min_samples_split=5,  \n",
    "    min_samples_leaf=2,   \n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Random Forest model with balanced class weights...\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation to get more robust performance estimate\n",
    "cv_scores = cross_val_score(clf, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='accuracy')\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"OOB Score: {clf.oob_score_:.4f}\")\n",
    "\n",
    "# Classification report with zero_division parameter to suppress warnings\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, y_pred, \n",
    "    target_names=class_names, \n",
    "    zero_division=0  \n",
    "))\n",
    "\n",
    "# Detailed per-class analysis\n",
    "print(\"\\nDetailed per-class analysis:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    true_count = np.sum(y_test == i)\n",
    "    pred_count = np.sum(y_pred == i)\n",
    "    correct_count = np.sum((y_test == i) & (y_pred == i))\n",
    "    \n",
    "    if true_count > 0:\n",
    "        recall = correct_count / true_count\n",
    "    else:\n",
    "        recall = 0\n",
    "        \n",
    "    if pred_count > 0:\n",
    "        precision = correct_count / pred_count\n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "    print(f\"  {class_name}:\")\n",
    "    print(f\"    True samples: {true_count}, Predicted: {pred_count}, Correct: {correct_count}\")\n",
    "    print(f\"    Precision: {precision:.3f}, Recall: {recall:.3f}\")\n",
    "\n",
    "# Confusion Matrix in percentages\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "# Replace NaN values (from division by zero) with 0\n",
    "cm_percent = np.nan_to_num(cm_percent, nan=0.0)\n",
    "sns.heatmap(cm_percent, annot=True, fmt='.1f', xticklabels=class_names, yticklabels=class_names, \n",
    "            cmap=\"Blues\", cbar_kws={'label': 'Percentage (%)'})\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (in percent)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_percent_0.2.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "\n",
    "# Most Important Features\n",
    "plt.figure(figsize=(12, 8))\n",
    "importances.nlargest(15).plot(kind='barh', color='teal')\n",
    "plt.title(\"Top 15 Most Important Features\")\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Least Important Features\n",
    "plt.figure(figsize=(12, 8))\n",
    "importances.nsmallest(15).plot(kind='barh', color='coral')\n",
    "plt.title(\"Top 15 Least Important Features\")\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "for i, (feature, importance) in enumerate(importances.nlargest(10).items(), 1):\n",
    "    print(f\"{i:2d}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# Print least important features\n",
    "print(f\"\\nTop 10 Least Important Features:\")\n",
    "for i, (feature, importance) in enumerate(importances.nsmallest(10).items(), 1):\n",
    "    print(f\"{i:2d}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# Analyze prediction confidence\n",
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "max_probabilities = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "print(f\"\\nPrediction confidence analysis:\")\n",
    "print(f\"Mean prediction confidence: {max_probabilities.mean():.3f}\")\n",
    "print(f\"Min prediction confidence: {max_probabilities.min():.3f}\")\n",
    "print(f\"Max prediction confidence: {max_probabilities.max():.3f}\")\n",
    "\n",
    "# Find low-confidence predictions\n",
    "low_confidence_threshold = 0.5\n",
    "low_confidence_mask = max_probabilities < low_confidence_threshold\n",
    "if np.any(low_confidence_mask):\n",
    "    print(f\"\\nFound {np.sum(low_confidence_mask)} predictions with confidence < {low_confidence_threshold}\")\n",
    "    print(\"These might be misclassified or difficult cases.\")\n",
    "\n",
    "# Save Model and encoders\n",
    "print(\"\\nSaving model and encoders...\")\n",
    "joblib.dump(clf, \"random_forest_model.pkl\")\n",
    "joblib.dump(le, \"label_encoder.pkl\")\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_names = X.columns.tolist()\n",
    "joblib.dump(feature_names, \"feature_names.pkl\")\n",
    "\n",
    "print(\"Model training and evaluation complete!\")\n",
    "print(f\"Files saved:\")\n",
    "print(f\"  - Model: random_forest_model.pkl\")\n",
    "print(f\"  - Label encoder: label_encoder.pkl\")\n",
    "print(f\"  - Feature names: feature_names.pkl\")\n",
    "print(f\"  - Confusion matrix: confusion_matrix_percent.png\")\n",
    "\n",
    "# Additional recommendations\n",
    "if rare_classes:\n",
    "    print(f\"\\nRecommendations:\")\n",
    "    print(f\"Consider collecting more data for classes: {rare_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88082bb-549a-4f1d-a04c-3875413502e3",
   "metadata": {},
   "source": [
    "# Trained Random Forest on a new dataset\n",
    "## Dataset mustn't have class names\n",
    "### File is called 2025_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c3633-9618-4296-abec-a6fd884dee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model again\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load model and encoder\n",
    "clf = joblib.load(\"random_forest_model.pkl\")\n",
    "le = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# Load new data to classify (must have the same feature columns)\n",
    "new_data = pd.read_csv(\"2025_features.csv\")  # No 'class_name' column\n",
    "\n",
    "# Predict\n",
    "predictions = clf.predict(new_data)\n",
    "\n",
    "# Decode predicted labels\n",
    "predicted_labels = le.inverse_transform(predictions)\n",
    "print(predicted_labels)\n",
    "\n",
    "# Save predictions to csv\n",
    "output = pd.DataFrame({\n",
    "    \"event_time\": new_data[\"event_time\"],\n",
    "    \"predicted_event\": predicted_labels\n",
    "})\n",
    "output.to_csv(\"20205_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
